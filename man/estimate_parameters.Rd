% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimate_parameters.R, R/modeling_phrases.R
\name{estimate_parameters.lm}
\alias{estimate_parameters.lm}
\alias{estimate_parameters.glm}
\alias{estimate_parameters}
\title{Estimate the parameters of linear or generalized linear models.}
\usage{
\method{estimate_parameters}{lm}(
  mean.model,
  confidence.level,
  simulation.replications = 4999,
  assume.identically.distributed = FALSE,
  assume.constant.variance = assume.identically.distributed,
  assume.normality = FALSE,
  construct = c("normal-2", "normal-1", "two-point mass"),
  type = c("percentile", "BC", "bootstrap-t"),
  ...
)

\method{estimate_parameters}{glm}(
  mean.model,
  confidence.level,
  simulation.replications = 4999,
  method = c("classical", "parametric", "case-resampling"),
  type = c("percentile", "BC", "bootstrap-t"),
  ...
)

estimate_parameters(
  mean.model,
  confidence.level,
  simulation.replications = 4999,
  ...
)
}
\arguments{
\item{mean.model}{\code{lm} or \code{glm} model fit defining the model
therefore the parameters of the mean model to be estimated.}

\item{confidence.level}{scalar between 0 and 1 indicating the confidence
level for all confidence intervals constructed. If missing (default), only
point estimates are returned.}

\item{simulation.replications}{scalar indicating the number of samples to
draw from the model for the sampling distribution (default = 4999). This will
either be the number of bootstrap replications or the number of samples from
the classical sampling distribution. This is ignored if
\code{confidence.level} is not specified.}

\item{assume.identically.distributed}{boolean; if \code{TRUE},
homoskedasticity is assumed for the error term. If \code{FALSE} (default),
this is not assumed.}

\item{assume.constant.variance}{another way of specifying
\code{assume.identically.distributed}. Both should not be specified.}

\item{assume.normality}{boolean; if \code{TRUE}, the errors are assumed to
follow a Normal distribution. If \code{FALSE} (default), this is not
assumed.}

\item{construct}{string defining the type of construct to use when generating
from the distribution for the wild bootrap (see \code{\link{rmammen}}). If
\code{assume.constant.variance = TRUE}, this is ignored
(default = \code{"normal-2"}).}

\item{type}{string defining the type of confidence interval to construct. If
\code{"percentile"} (default) an equal-tailed percentile interval is
constructed. If \code{"BC"} the bias-corrected percentile interval is
constructed. If \code{"bootstrap-t"} the bootstrap-t interval is constructed.}

\item{...}{additional arguments to be passed to other methods.}

\item{method}{string defining the methodology to employ. If
\code{"classical"} (default), the model is assumed correct and classical
large-sample theory is used. If \code{"parametric"}, a parametric bootstrap
is performed. If \code{"case-resampling"}, a case-resampling bootstrap is
performed.}
}
\value{
data.frame containing a table of parameter estimates. The object
has an additional attribute "Sampling Distribution" which is a matrix with
\code{simulation.replications} rows and the same number of
columns as parameters in \code{mean.model}.  Each column contains a sample
from the corresponding model of the sampling distribution. This is useful for
plotting the modeled sampling distribution.
}
\description{
Provides point estimates and confidence intervals for the parameters of a
linear or generalized linear model via bootstrapping or classical theory.
}
\details{
This wrapper provides a single interface for estimating parameters under
various conditions imposed on the model. Similar to
\code{\link[base]{summary}}, point and interval estimates of the parameters
are available. However, interval estimates can be constructed via
bootstrapping or classical theory.

For linear models, the following approaches are implemented:
\itemize{
  \item classical: if both homoskedasticity and normality are assumed, the
sampling distributions of a standardized statistic is modeled by a
t-distribution.
  \item parametric bootstrap: if normality can be assumed but
homoskedasticity cannot, a parametric bootstrap can be peformed in which the
variance for each observation is estimated by the square of the corresponding
residual (similar to a White's correction).
  \item residual bootstrap: if homoskedasticity can be assumed, but normality
cannot, a residual bootstrap is used to compute standard errors and
confidence intervals.
  \item wild bootstrap: if neither homoskedasticity nor normality is assumed,
a wild bootstrap is used to compute standard errors and confidence intervals.
}
All methods make additional requirements regarding independence of the error
terms and that the model has been correctly specified. Note: for parametric
bootstrap assuming constant variance, use a generalized linear model
approach.

For generalized linear models, the following approaches are implemented:
\itemize{
  \item classical: if the distributional family is assumed correct, large
  sample theory is used to justify modeling the sampling distribution of a
  standardized statistic using a standard normal distribution.
  \item parametric bootstrap: the distributional family is assumed and
  a parametric bootstrap is performed to compute standard errors and
  confidence intervals.
  \item nonparametric bootstrap: a case resampling bootstrap algorithm is
  used to estimate standard errors and confidence intervals.
}
All methods require observations to be independent of one another.

Confidence intervals constructed via bootstrapping can take on various forms.
The percentile interval is constructed by taking the empirical
\eqn{100\alpha} and \eqn{100(1-\alpha)} percentiles from the bootstrap
statistics. If \eqn{\hat{F}} is the empirical distribution function of the
bootstrap values, then the \eqn{100(1 - 2\alpha)}% percentile interval is
given by
\deqn{(\hat{F}^{-1}(\alpha), \hat{F}^{-1}(1-\alpha))}
The bias-corrected (BC) interval corrects for median-bias.  It is given by
\deqn{(\hat{F}^{-1}(\alpha_1), \hat{F}^{-1}(1-\alpha_2))}
where
\deqn{\alpha_1 = \Phi{2\hat{z}_0 + \Phi^{-1}(\alpha)}}
\deqn{\alpha_2 = 1 - \Phi{2\hat{z}_0 + \Phi^{-1}(1-\alpha)}}
\deqn{\hat{z}_0 = \Phi^{-1}(\hat{F}(\hat{\beta}))}
where \eqn{\hat{\beta}} is the estimate from the original sample.
The bootstrap-t interval is based on the bootstrap distribution of
\deqn{t^{b} = \frac{\hat{\beta}^{b} -
\hat{\beta}}{\hat{\sigma}^{b}}}
where \eqn{\hat{\sigma}} is the estimate of the standard error of
\eqn{\hat{\beta}} and the superscript b denotes a bootstrap sample. Let
\eqn{\hat{G}} be the empirical distribution function of the bootstrap
standardized statistics given above.  Then, the bootstrap-t interval is given
by
\deqn{(\hat{\beta} - \hat{\sigma}\hat{G}^{-1}(1-\alpha),
\hat{\beta} - \hat{\sigma}\hat{G}^{-1}\alpha)}
}
\section{Methods (by class)}{
\itemize{
\item \code{estimate_parameters(lm)}: Estimates for linear models.

\item \code{estimate_parameters(glm)}: Estimates for generalized linear models.

}}
\examples{
fit <- lm(mpg ~ 1 + hp, data = mtcars)

# confidence intervals for linear model via a residual bootstrap
estimate_parameters(fit,
  confidence.level = 0.95,
  assume.identically.distributed = TRUE,
  assume.normality = FALSE)

# classical inference
estimate_parameters(fit,
  confidence.level = 0.95,
  assume.identically.distributed = TRUE,
  assume.normality = TRUE)

}
